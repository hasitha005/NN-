import numpy as np

# ---------- 1. Input data ----------
x = np.array([1.0, 2.0])      # 2 inputs
y = 1                         # label (0 or 1)

# ---------- 2. Initialize weights ----------
W1 = np.array([[0.2, -0.1],
               [0.4,  0.3]])   # weights: input → hidden (2x2)
b1 = np.array([0.1, -0.2])     # bias for hidden layer

W2 = np.array([[0.5],
               [-0.4]])        # weights: hidden → output (2x1)
b2 = np.array([0.2])           # bias for output layer

# ---------- Activation functions ----------
def relu(z):
    return np.maximum(0, z)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# ---------- 3. Forward pass ----------
# Hidden layer
z1 = np.dot(x, W1) + b1
a1 = relu(z1)

# Output layer
z2 = np.dot(a1, W2) + b2
y_hat = sigmoid(z2)

print("Input:", x)
print("Hidden pre-activation:", z1)
print("Hidden output:", a1)
print("Final output:", y_hat)
